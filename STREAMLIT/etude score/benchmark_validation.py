#!/usr/bin/env python3
"""
============================================================
VALIDATION ET BENCHMARK DU M√âTA-SCORE QeV
Tests de Fiabilit√© et Robustesse
============================================================

M√©thodes de validation:
1. Analyse de sensibilit√© (variation des poids)
2. Test de coh√©rence interne (corr√©lations)
3. Test de capacit√© discriminante
4. Validation crois√©e avec indices existants
5. Tests de robustesse aux valeurs extr√™mes

R√©f√©rences:
- Saisana & Tarantola (2002): State-of-the-art report on composite indicators
- Saltelli et al. (2008): Global Sensitivity Analysis
============================================================
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple
import logging
from scipy.stats import pearsonr, spearmanr
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score

# Import du calculateur principal
from metascore_calculator import (
    QeVCalculator, QeVConfig, QeVSimulator,
    TrafficData, GreenSpaceData, AirQualityData, QeVScore
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================
# CLASSE DE VALIDATION
# ============================================================

class QeVValidator:
    """Valide et teste la robustesse du m√©ta-score QeV"""
    
    def __init__(self, calculator: QeVCalculator):
        """
        Initialise le validateur
        
        Args:
            calculator: Instance du calculateur QeV
        """
        self.calculator = calculator
        self.baseline_config = calculator.config
        self.validation_results = {}
    
    def sensitivity_analysis(
        self,
        scenarios: List[Tuple[TrafficData, GreenSpaceData, AirQualityData]],
        weight_variations: List[float] = None
    ) -> Dict:
        """
        Analyse de sensibilit√©: teste l'impact de la variation des poids
        
        Objectif: V√©rifier si de petites modifications des poids changent
        drastiquement le classement des zones (signe d'instabilit√©)
        
        Args:
            scenarios: Liste de tuples (traffic, green, air)
            weight_variations: Liste de variations √† tester (ex: [-0.2, -0.1, 0, 0.1, 0.2])
            
        Returns:
            Dictionnaire avec r√©sultats d'analyse
        """
        logger.info("\n" + "="*60)
        logger.info("üî¨ ANALYSE DE SENSIBILIT√â")
        logger.info("="*60 + "\n")
        
        if weight_variations is None:
            weight_variations = [-0.2, -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.15, 0.2]
        
        baseline_weights = self.baseline_config.GLOBAL_WEIGHTS.copy()
        baseline_scores = []
        
        # Calculer scores de r√©f√©rence
        logger.info("üìä Calcul des scores de r√©f√©rence...")
        for traffic, green, air in scenarios:
            score = self.calculator.calculate_qev_score(traffic, green, air)
            baseline_scores.append(score.qev_score)
        
        baseline_ranking = np.argsort(baseline_scores)[::-1]  # Tri d√©croissant
        
        results = {
            'baseline_scores': baseline_scores,
            'baseline_ranking': baseline_ranking,
            'variations': {},
            'ranking_changes': [],
            'score_changes': []
        }
        
        # Tester chaque variation
        logger.info(f"üß™ Test de {len(weight_variations)} variations de poids...\n")
        
        for variation in weight_variations:
            if variation == 0:
                continue
            
            # Varier le poids 'air' et ajuster les autres proportionnellement
            new_air_weight = baseline_weights['air'] + variation
            
            # Contraintes: poids entre 0 et 1, somme = 1
            if new_air_weight <= 0 or new_air_weight >= 1:
                continue
            
            remaining = 1.0 - new_air_weight
            ratio = remaining / (baseline_weights['traffic'] + baseline_weights['green'])
            
            new_weights = {
                'air': new_air_weight,
                'traffic': baseline_weights['traffic'] * ratio,
                'green': baseline_weights['green'] * ratio
            }
            
            # Cr√©er une nouvelle config avec ces poids
            test_config = QeVConfig()
            test_config.GLOBAL_WEIGHTS = new_weights
            test_calculator = QeVCalculator(test_config)
            
            # Recalculer tous les scores
            varied_scores = []
            for traffic, green, air in scenarios:
                score = test_calculator.calculate_qev_score(traffic, green, air)
                varied_scores.append(score.qev_score)
            
            varied_ranking = np.argsort(varied_scores)[::-1]
            
            # Calculer les changements
            ranking_diff = np.sum(np.abs(baseline_ranking - varied_ranking))
            score_mae = mean_absolute_error(baseline_scores, varied_scores)
            max_score_change = np.max(np.abs(np.array(baseline_scores) - np.array(varied_scores)))
            
            results['variations'][f'{variation:+.2f}'] = {
                'weights': new_weights,
                'scores': varied_scores,
                'ranking': varied_ranking,
                'ranking_difference': ranking_diff,
                'mae': score_mae,
                'max_change': max_score_change
            }
            
            results['ranking_changes'].append(ranking_diff)
            results['score_changes'].append(score_mae)
            
            logger.info(f"Variation {variation:+.2f}: "
                       f"Œî ranking={ranking_diff}, MAE={score_mae:.4f}, "
                       f"Max Œî={max_score_change:.4f}")
        
        # Calculer statistiques de robustesse
        if results['ranking_changes']:
            avg_ranking_change = np.mean(results['ranking_changes'])
            max_ranking_change = np.max(results['ranking_changes'])
            avg_score_mae = np.mean(results['score_changes'])
            
            results['robustness_metrics'] = {
                'avg_ranking_change': avg_ranking_change,
                'max_ranking_change': max_ranking_change,
                'avg_score_mae': avg_score_mae,
                'is_robust': avg_ranking_change < len(scenarios) * 0.2  # <20% changement
            }
            
            logger.info(f"\nüìà R√©sultats:")
            logger.info(f"   Changement moyen de rang: {avg_ranking_change:.1f}")
            logger.info(f"   Changement max de rang: {max_ranking_change:.0f}")
            logger.info(f"   MAE moyen des scores: {avg_score_mae:.4f}")
            
            if results['robustness_metrics']['is_robust']:
                logger.info("   ‚úÖ Le mod√®le est ROBUSTE aux variations de poids")
            else:
                logger.info("   ‚ö†Ô∏è  Le mod√®le est SENSIBLE aux variations de poids")
        
        self.validation_results['sensitivity'] = results
        return results
    
    def internal_consistency_test(self, scores: List[QeVScore]) -> Dict:
        """
        Test de coh√©rence interne: v√©rifie les corr√©lations entre composantes
        
        Objectif: S'assurer que les sous-indices ne sont pas trop corr√©l√©s
        (redondance) ni trop ind√©pendants (incoh√©rence)
        
        Args:
            scores: Liste de scores QeV calcul√©s
            
        Returns:
            Dictionnaire avec corr√©lations et diagnostics
        """
        logger.info("\n" + "="*60)
        logger.info("üî¨ TEST DE COH√âRENCE INTERNE")
        logger.info("="*60 + "\n")
        
        # Extraire les composantes
        air_scores = [s.air_score for s in scores]
        traffic_scores = [s.traffic_score for s in scores]
        green_scores = [s.green_score for s in scores]
        qev_scores = [s.qev_score for s in scores]
        
        # Cr√©er DataFrame pour faciliter l'analyse
        df = pd.DataFrame({
            'Air': air_scores,
            'Trafic': traffic_scores,
            'Vert': green_scores,
            'QeV': qev_scores
        })
        
        # Calculer matrice de corr√©lation
        corr_pearson = df.corr(method='pearson')
        corr_spearman = df.corr(method='spearman')
        
        logger.info("üìä Matrice de corr√©lation (Pearson):")
        logger.info(corr_pearson.to_string())
        logger.info("")
        
        # Analyser les corr√©lations entre sous-indices
        results = {
            'correlation_pearson': corr_pearson.to_dict(),
            'correlation_spearman': corr_spearman.to_dict(),
            'diagnostics': {}
        }
        
        # Test 1: Multicolin√©arit√© entre Air et Trafic
        air_traffic_corr = corr_pearson.loc['Air', 'Trafic']
        results['diagnostics']['air_traffic_collinearity'] = {
            'correlation': air_traffic_corr,
            'is_problematic': abs(air_traffic_corr) > 0.9,
            'interpretation': (
                "Forte multicolin√©arit√© (>0.9)" if abs(air_traffic_corr) > 0.9
                else "Multicolin√©arit√© acceptable (<0.9)"
            )
        }
        
        logger.info(f"üîç Corr√©lation Air-Trafic: {air_traffic_corr:.3f}")
        logger.info(f"   ‚Üí {results['diagnostics']['air_traffic_collinearity']['interpretation']}")
        
        # Test 2: Contribution √©quilibr√©e au score final
        contributions = {
            'Air': abs(corr_pearson.loc['Air', 'QeV']),
            'Trafic': abs(corr_pearson.loc['Trafic', 'QeV']),
            'Vert': abs(corr_pearson.loc['Vert', 'QeV'])
        }
        
        max_contrib = max(contributions.values())
        min_contrib = min(contributions.values())
        contrib_ratio = max_contrib / min_contrib if min_contrib > 0 else float('inf')
        
        results['diagnostics']['contribution_balance'] = {
            'contributions': contributions,
            'ratio': contrib_ratio,
            'is_balanced': contrib_ratio < 3.0,
            'interpretation': (
                "Contributions √©quilibr√©es" if contrib_ratio < 3.0
                else "Une composante domine excessivement"
            )
        }
        
        logger.info(f"\nüîç Contributions au QeV:")
        for comp, val in contributions.items():
            logger.info(f"   {comp}: {val:.3f}")
        logger.info(f"   Ratio max/min: {contrib_ratio:.2f}")
        logger.info(f"   ‚Üí {results['diagnostics']['contribution_balance']['interpretation']}")
        
        # Test 3: Variance expliqu√©e
        from sklearn.linear_model import LinearRegression
        
        X = df[['Air', 'Trafic', 'Vert']].values
        y = df['QeV'].values
        
        model = LinearRegression()
        model.fit(X, y)
        r2 = model.score(X, y)
        
        results['diagnostics']['variance_explained'] = {
            'r2': r2,
            'interpretation': (
                "Excellent (>0.95)" if r2 > 0.95
                else "Bon (>0.90)" if r2 > 0.90
                else "Acceptable (>0.80)" if r2 > 0.80
                else "Faible (<0.80)"
            )
        }
        
        logger.info(f"\nüîç Variance expliqu√©e (R¬≤): {r2:.4f}")
        logger.info(f"   ‚Üí {results['diagnostics']['variance_explained']['interpretation']}")
        
        self.validation_results['consistency'] = results
        return results
    
    def discriminant_power_test(self, scores: List[QeVScore]) -> Dict:
        """
        Test de capacit√© discriminante: v√©rifie si le score diff√©rencie bien les zones
        
        Objectif: S'assurer que le score n'est pas trop "liss√©" ou "extr√™me"
        
        Args:
            scores: Liste de scores QeV
            
        Returns:
            Dictionnaire avec m√©triques de discrimination
        """
        logger.info("\n" + "="*60)
        logger.info("üî¨ TEST DE CAPACIT√â DISCRIMINANTE")
        logger.info("="*60 + "\n")
        
        qev_values = [s.qev_score for s in scores]
        
        # Statistiques descriptives
        mean_score = np.mean(qev_values)
        std_score = np.std(qev_values)
        cv = std_score / mean_score if mean_score > 0 else 0  # Coefficient de variation
        score_range = np.max(qev_values) - np.min(qev_values)
        
        # Distribution par cat√©gories
        categories = {}
        for score in scores:
            categories[score.category] = categories.get(score.category, 0) + 1
        
        # Calculer l'entropie de Shannon (mesure de diversit√©)
        total = len(scores)
        entropy = -sum((count/total) * np.log2(count/total) 
                      for count in categories.values())
        max_entropy = np.log2(len(categories)) if categories else 0
        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
        
        results = {
            'mean': mean_score,
            'std': std_score,
            'cv': cv,
            'range': score_range,
            'categories': categories,
            'entropy': entropy,
            'normalized_entropy': normalized_entropy,
            'diagnostics': {}
        }
        
        # Diagnostic 1: Coefficient de variation
        results['diagnostics']['variation'] = {
            'cv': cv,
            'is_adequate': 0.15 < cv < 0.40,
            'interpretation': (
                "Bonne discrimination" if 0.15 < cv < 0.40
                else "Trop homog√®ne (<0.15)" if cv <= 0.15
                else "Trop h√©t√©rog√®ne (>0.40)"
            )
        }
        
        logger.info(f"üìä Statistiques:")
        logger.info(f"   Moyenne: {mean_score:.3f}")
        logger.info(f"   √âcart-type: {std_score:.3f}")
        logger.info(f"   Coefficient de variation: {cv:.3f}")
        logger.info(f"   √âtendue: {score_range:.3f}")
        logger.info(f"   ‚Üí {results['diagnostics']['variation']['interpretation']}")
        
        # Diagnostic 2: Distribution par cat√©gories
        results['diagnostics']['distribution'] = {
            'entropy': normalized_entropy,
            'is_diverse': normalized_entropy > 0.6,
            'interpretation': (
                "Distribution diverse" if normalized_entropy > 0.6
                else "Distribution concentr√©e"
            )
        }
        
        logger.info(f"\nüìä Distribution par cat√©gories:")
        for cat, count in sorted(categories.items()):
            pct = (count / total) * 100
            logger.info(f"   {cat}: {count} ({pct:.1f}%)")
        logger.info(f"   Entropie normalis√©e: {normalized_entropy:.3f}")
        logger.info(f"   ‚Üí {results['diagnostics']['distribution']['interpretation']}")
        
        # Diagnostic 3: Capacit√© √† s√©parer extr√™mes
        bottom_10 = np.percentile(qev_values, 10)
        top_10 = np.percentile(qev_values, 90)
        separation = top_10 - bottom_10
        
        results['diagnostics']['separation'] = {
            'percentile_10': bottom_10,
            'percentile_90': top_10,
            'separation': separation,
            'is_adequate': separation > 0.3,
            'interpretation': (
                "Bonne s√©paration (>0.3)" if separation > 0.3
                else "S√©paration faible (<0.3)"
            )
        }
        
        logger.info(f"\nüìä S√©paration des extr√™mes:")
        logger.info(f"   10e percentile: {bottom_10:.3f}")
        logger.info(f"   90e percentile: {top_10:.3f}")
        logger.info(f"   √âcart: {separation:.3f}")
        logger.info(f"   ‚Üí {results['diagnostics']['separation']['interpretation']}")
        
        self.validation_results['discriminant'] = results
        return results
    
    def extreme_values_test(
        self,
        scenarios: List[Tuple[TrafficData, GreenSpaceData, AirQualityData]]
    ) -> Dict:
        """
        Test de robustesse aux valeurs extr√™mes
        
        Objectif: V√©rifier que le score ne produit pas de r√©sultats absurdes
        avec des valeurs extr√™mes
        
        Args:
            scenarios: Liste de sc√©narios normaux
            
        Returns:
            R√©sultats des tests
        """
        logger.info("\n" + "="*60)
        logger.info("üî¨ TEST DE ROBUSTESSE AUX VALEURS EXTR√äMES")
        logger.info("="*60 + "\n")
        
        results = {
            'baseline': [],
            'extreme_tests': []
        }
        
        # Calculer scores de r√©f√©rence
        for traffic, green, air in scenarios[:3]:  # Prendre 3 exemples
            score = self.calculator.calculate_qev_score(traffic, green, air)
            results['baseline'].append({
                'location': score.location,
                'qev': score.qev_score
            })
        
        # Test 1: Pollution maximale
        logger.info("üß™ Test 1: Pollution maximale")
        extreme_air = AirQualityData(
            no2_concentration=100,
            pm25_concentration=50,
            pm10_concentration=100,
            location="Zone Extr√™me - Pollution Max"
        )
        extreme_traffic = TrafficData(5000, 500, 200, "Zone Extr√™me")
        minimal_green = GreenSpaceData(1000, 0, "Zone Extr√™me")
        
        worst_score = self.calculator.calculate_qev_score(
            extreme_traffic, minimal_green, extreme_air
        )
        
        results['extreme_tests'].append({
            'test': 'pollution_max',
            'qev': worst_score.qev_score,
            'is_valid': 0.0 <= worst_score.qev_score <= 0.3,
            'interpretation': (
                "Score coh√©rent (proche de 0)" if worst_score.qev_score <= 0.3
                else "‚ö†Ô∏è Score trop √©lev√© pour conditions extr√™mes"
            )
        })
        
        logger.info(f"   Score QeV: {worst_score.qev_score:.3f}")
        logger.info(f"   ‚Üí {results['extreme_tests'][-1]['interpretation']}")
        
        # Test 2: Conditions id√©ales
        logger.info("\nüß™ Test 2: Conditions id√©ales")
        perfect_air = AirQualityData(
            no2_concentration=5,
            pm25_concentration=3,
            pm10_concentration=8,
            location="Zone Id√©ale - Conditions Parfaites"
        )
        minimal_traffic = TrafficData(50, 5, 0, "Zone Id√©ale")
        maximal_green = GreenSpaceData(500000, 100, "Zone Id√©ale")
        
        best_score = self.calculator.calculate_qev_score(
            minimal_traffic, maximal_green, perfect_air
        )
        
        results['extreme_tests'].append({
            'test': 'conditions_ideal',
            'qev': best_score.qev_score,
            'is_valid': 0.7 <= best_score.qev_score <= 1.0,
            'interpretation': (
                "Score coh√©rent (proche de 1)" if best_score.qev_score >= 0.7
                else "‚ö†Ô∏è Score trop faible pour conditions id√©ales"
            )
        })
        
        logger.info(f"   Score QeV: {best_score.qev_score:.3f}")
        logger.info(f"   ‚Üí {results['extreme_tests'][-1]['interpretation']}")
        
        # Test 3: √âcart entre extr√™mes
        extreme_range = best_score.qev_score - worst_score.qev_score
        
        results['extreme_range'] = {
            'range': extreme_range,
            'is_adequate': extreme_range > 0.5,
            'interpretation': (
                "Bonne discrimination extr√™mes (>0.5)" if extreme_range > 0.5
                else "‚ö†Ô∏è Discrimination insuffisante (<0.5)"
            )
        }
        
        logger.info(f"\nüìä √âcart entre extr√™mes: {extreme_range:.3f}")
        logger.info(f"   ‚Üí {results['extreme_range']['interpretation']}")
        
        self.validation_results['extreme'] = results
        return results
    
    def generate_validation_report(self, output_file: str = "validation_report.txt"):
        """
        G√©n√®re un rapport complet de validation
        
        Args:
            output_file: Nom du fichier de sortie
        """
        if not self.validation_results:
            logger.warning("‚ö†Ô∏è  Aucun r√©sultat de validation disponible")
            return
        
        report_path = Path(__file__).parent / output_file
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("RAPPORT DE VALIDATION - M√âTA-SCORE QeV\n")
            f.write("Tests de Fiabilit√© et Robustesse\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"Date: {pd.Timestamp.now().strftime('%d/%m/%Y %H:%M:%S')}\n\n")
            
            # 1. Analyse de sensibilit√©
            if 'sensitivity' in self.validation_results:
                f.write("-" * 80 + "\n")
                f.write("1. ANALYSE DE SENSIBILIT√â\n")
                f.write("-" * 80 + "\n\n")
                
                sens = self.validation_results['sensitivity']
                
                if 'robustness_metrics' in sens:
                    metrics = sens['robustness_metrics']
                    f.write("R√©sum√©:\n")
                    f.write(f"  ‚Ä¢ Changement moyen de rang: {metrics['avg_ranking_change']:.2f}\n")
                    f.write(f"  ‚Ä¢ Changement max de rang: {metrics['max_ranking_change']:.0f}\n")
                    f.write(f"  ‚Ä¢ MAE moyen des scores: {metrics['avg_score_mae']:.4f}\n")
                    f.write(f"  ‚Ä¢ Mod√®le robuste: {'‚úÖ OUI' if metrics['is_robust'] else '‚ö†Ô∏è NON'}\n\n")
                    
                    f.write("Interpr√©tation:\n")
                    if metrics['is_robust']:
                        f.write("Le mod√®le est ROBUSTE. Les variations de poids (¬±20%) n'affectent\n")
                        f.write("pas significativement le classement des zones. Cela d√©montre que les\n")
                        f.write("conclusions sont fiables et ne d√©pendent pas excessivement des choix\n")
                        f.write("de pond√©ration.\n\n")
                    else:
                        f.write("‚ö†Ô∏è Le mod√®le montre une certaine SENSIBILIT√â aux poids. Il est recommand√©\n")
                        f.write("de justifier les pond√©rations choisies par la litt√©rature ou par un\n")
                        f.write("consensus d'experts (m√©thode Delphi).\n\n")
            
            # 2. Coh√©rence interne
            if 'consistency' in self.validation_results:
                f.write("-" * 80 + "\n")
                f.write("2. COH√âRENCE INTERNE\n")
                f.write("-" * 80 + "\n\n")
                
                cons = self.validation_results['consistency']
                diag = cons['diagnostics']
                
                # Multicolin√©arit√©
                if 'air_traffic_collinearity' in diag:
                    mc = diag['air_traffic_collinearity']
                    f.write(f"a) Multicolin√©arit√© Air-Trafic: {mc['correlation']:.3f}\n")
                    f.write(f"   Statut: {mc['interpretation']}\n\n")
                
                # Contributions
                if 'contribution_balance' in diag:
                    cb = diag['contribution_balance']
                    f.write("b) Contributions au score final:\n")
                    for comp, val in cb['contributions'].items():
                        f.write(f"   ‚Ä¢ {comp}: {val:.3f}\n")
                    f.write(f"   Ratio max/min: {cb['ratio']:.2f}\n")
                    f.write(f"   Statut: {cb['interpretation']}\n\n")
                
                # Variance expliqu√©e
                if 'variance_explained' in diag:
                    ve = diag['variance_explained']
                    f.write(f"c) Variance expliqu√©e (R¬≤): {ve['r2']:.4f}\n")
                    f.write(f"   Statut: {ve['interpretation']}\n\n")
            
            # 3. Capacit√© discriminante
            if 'discriminant' in self.validation_results:
                f.write("-" * 80 + "\n")
                f.write("3. CAPACIT√â DISCRIMINANTE\n")
                f.write("-" * 80 + "\n\n")
                
                disc = self.validation_results['discriminant']
                diag = disc['diagnostics']
                
                # Variation
                if 'variation' in diag:
                    var = diag['variation']
                    f.write(f"a) Coefficient de variation: {var['cv']:.3f}\n")
                    f.write(f"   Statut: {var['interpretation']}\n\n")
                
                # Distribution
                if 'distribution' in diag:
                    dist = diag['distribution']
                    f.write(f"b) Entropie normalis√©e: {dist['entropy']:.3f}\n")
                    f.write(f"   Statut: {dist['interpretation']}\n\n")
                
                # S√©paration
                if 'separation' in diag:
                    sep = diag['separation']
                    f.write(f"c) S√©paration extr√™mes (P90-P10): {sep['separation']:.3f}\n")
                    f.write(f"   Statut: {sep['interpretation']}\n\n")
            
            # 4. Valeurs extr√™mes
            if 'extreme' in self.validation_results:
                f.write("-" * 80 + "\n")
                f.write("4. ROBUSTESSE AUX VALEURS EXTR√äMES\n")
                f.write("-" * 80 + "\n\n")
                
                ext = self.validation_results['extreme']
                
                for test in ext['extreme_tests']:
                    f.write(f"{test['test']}: Score = {test['qev']:.3f}\n")
                    f.write(f"  ‚Üí {test['interpretation']}\n\n")
                
                if 'extreme_range' in ext:
                    er = ext['extreme_range']
                    f.write(f"√âcart entre extr√™mes: {er['range']:.3f}\n")
                    f.write(f"  ‚Üí {er['interpretation']}\n\n")
            
            # Conclusion
            f.write("-" * 80 + "\n")
            f.write("5. CONCLUSION G√âN√âRALE\n")
            f.write("-" * 80 + "\n\n")
            
            # Compiler les statuts
            all_valid = True
            issues = []
            
            if 'sensitivity' in self.validation_results:
                if not self.validation_results['sensitivity'].get('robustness_metrics', {}).get('is_robust', True):
                    all_valid = False
                    issues.append("Sensibilit√© aux poids de pond√©ration")
            
            if all_valid:
                f.write("‚úÖ Le m√©ta-score QeV passe TOUS les tests de validation.\n\n")
                f.write("Le mod√®le est:\n")
                f.write("‚Ä¢ Robuste aux variations de param√®tres\n")
                f.write("‚Ä¢ Coh√©rent dans sa structure interne\n")
                f.write("‚Ä¢ Capable de discriminer efficacement les zones\n")
                f.write("‚Ä¢ Fiable avec des valeurs extr√™mes\n\n")
                f.write("Ce score peut √™tre utilis√© en toute confiance pour l'analyse\n")
                f.write("de la qualit√© environnementale urbaine.\n")
            else:
                f.write("‚ö†Ô∏è Le m√©ta-score QeV pr√©sente quelques points d'attention:\n\n")
                for issue in issues:
                    f.write(f"‚Ä¢ {issue}\n")
                f.write("\nRecommandations:\n")
                f.write("‚Ä¢ Justifier les pond√©rations par la litt√©rature\n")
                f.write("‚Ä¢ Effectuer une analyse de sensibilit√© dans le rapport\n")
                f.write("‚Ä¢ Discuter les limites en transparence\n")
            
            f.write("\n" + "=" * 80 + "\n")
        
        logger.info(f"‚úÖ Rapport de validation g√©n√©r√©: {report_path}")


# ============================================================
# FONCTION PRINCIPALE
# ============================================================

def main():
    """Point d'entr√©e principal du script de validation"""
    
    print("\n" + "="*80)
    print("VALIDATION ET BENCHMARK DU M√âTA-SCORE QeV")
    print("Tests de Fiabilit√© et Robustesse")
    print("="*80 + "\n")
    
    # Cr√©er calculateur et simulateur
    calculator = QeVCalculator()
    simulator = QeVSimulator(calculator)
    validator = QeVValidator(calculator)
    
    # G√©n√©rer des sc√©narios de test
    logger.info("üìä G√©n√©ration de sc√©narios de test...\n")
    
    test_scenarios = []
    for i in range(20):
        # Cr√©er des sc√©narios vari√©s
        traffic_level = np.random.uniform(100, 2000)
        green_level = np.random.uniform(0.2, 0.9)
        air_level = np.random.uniform(10, 80)
        
        traffic = TrafficData(
            cars=traffic_level,
            vans=traffic_level * 0.15,
            trucks=traffic_level * 0.05,
            location=f"Zone Test {i+1}"
        )
        
        green = GreenSpaceData(
            green_surface_m2_per_km2=green_level * 500000,
            trees_within_150m=int(green_level * 80),
            location=f"Zone Test {i+1}"
        )
        
        air = AirQualityData(
            no2_concentration=air_level,
            pm25_concentration=air_level * 0.4,
            pm10_concentration=air_level * 0.8,
            location=f"Zone Test {i+1}"
        )
        
        test_scenarios.append((traffic, green, air))
    
    # Calculer les scores
    logger.info("üßÆ Calcul des scores pour validation...\n")
    test_scores = []
    for traffic, green, air in test_scenarios:
        score = calculator.calculate_qev_score(traffic, green, air)
        test_scores.append(score)
    
    # Ex√©cuter les tests de validation
    logger.info("üî¨ Ex√©cution des tests de validation...\n")
    
    # 1. Analyse de sensibilit√©
    validator.sensitivity_analysis(test_scenarios)
    
    # 2. Coh√©rence interne
    validator.internal_consistency_test(test_scores)
    
    # 3. Capacit√© discriminante
    validator.discriminant_power_test(test_scores)
    
    # 4. Valeurs extr√™mes
    validator.extreme_values_test(test_scenarios)
    
    # G√©n√©rer rapport
    print("\n" + "-"*80)
    print("üìÑ G√âN√âRATION DU RAPPORT DE VALIDATION")
    print("-"*80 + "\n")
    
    validator.generate_validation_report("benchmark_validation_report.txt")
    
    print("\n" + "="*80)
    print("‚úÖ VALIDATION TERMIN√âE")
    print("="*80)
    print("\nüìÑ Fichier g√©n√©r√©: benchmark_validation_report.txt\n")


if __name__ == "__main__":
    main()
