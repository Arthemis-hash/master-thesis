#!/usr/bin/env python3
"""
============================================================
MODULE DE GESTION DES BASES DE DONN√âES - PostgreSQL/Prisma
============================================================
Remplacement de db_utils.py (SQLite) par version PostgreSQL
Gestion de DEUX bases de donn√©es distinctes via Prisma:
- AirQualityDB : Donn√©es de qualit√© de l'air
- WeatherDB : Donn√©es m√©t√©orologiques

Architecture: PostgreSQL + Prisma + PostGIS
============================================================
"""

# ============================================================
# IMPORTS
# ============================================================
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List
import pandas as pd

from prisma import Prisma
from prisma.models import Address, AirQualityRecord, WeatherRecord

logger = logging.getLogger(__name__)


# ============================================================
# CLIENT PRISMA SINGLETON
# ============================================================

class DatabaseClient:
    """Client Prisma singleton partag√©"""

    _instance: Optional[Prisma] = None
    _is_connected: bool = False
    _loop: Optional[object] = None

    @classmethod
    async def get_client(cls) -> Prisma:
        """R√©cup√®re ou cr√©e le client Prisma"""
        import asyncio
        current_loop = asyncio.get_running_loop()

        # Si le loop a chang√© (reload Streamlit), on doit recr√©er le client
        if cls._is_connected and cls._loop is not None and cls._loop is not current_loop:
            logger.warning("‚ö†Ô∏è Event loop changed (Streamlit reload?). Resetting Prisma client.")
            if cls._instance:
                try:
                    await cls._instance.disconnect()
                except Exception:
                    pass # Ignore errors on disconnect from dead loop
            cls._instance = None
            cls._is_connected = False

        if cls._instance is None:
            cls._instance = Prisma()

        if not cls._is_connected:
            await cls._instance.connect()
            cls._is_connected = True
            cls._loop = current_loop
            logger.info("‚úÖ Prisma DB client connected")

        return cls._instance

    @classmethod
    async def disconnect(cls):
        """Ferme la connexion"""
        if cls._instance and cls._is_connected:
            await cls._instance.disconnect()
            cls._is_connected = False
            cls._loop = None
            logger.info("üîå Prisma DB client disconnected")


# ============================================================
# GESTIONNAIRE D'ADRESSES
# ============================================================

class AddressManager:
    """Gestion des adresses g√©olocalis√©es"""

    def __init__(self):
        self.db: Optional[Prisma] = None

    async def _ensure_connected(self):
        if not self.db:
            self.db = await DatabaseClient.get_client()

    @staticmethod
    def sanitize_address(address: str) -> str:
        """Normalise une adresse"""
        import re

        # Si l'adresse est d√©j√† normalis√©e (commence par code_postal_), la retourner telle quelle
        if re.match(r'^\d{4,5}_', address):
            return address[:50]

        # Extraire code postal et ville
        parts = address.split(',')
        postal_code = None
        city = None

        for part in parts:
            part = part.strip()
            # Chercher code postal seulement au d√©but de la partie
            if not postal_code:
                postal_match = re.match(r'^(\d{4,5})\b', part)
                if postal_match:
                    postal_code = postal_match.group(1)

            city_keywords = ['bruxelles', 'brussels', 'brussel', 'ixelles', 'elsene',
                           'schaerbeek', 'etterbeek', 'anderlecht', 'molenbeek']
            if any(keyword in part.lower() for keyword in city_keywords):
                # Enlever le code postal du d√©but si pr√©sent
                city_part = re.sub(r'^(\d{4,5})\s*', '', part)
                city = re.sub(r'[^\w\s-]', '', city_part).strip()

        if postal_code and city:
            normalized = f"{postal_code}_{re.sub(r'\s+', '_', city.lower())}"
        elif postal_code:
            normalized = postal_code
        elif city:
            normalized = re.sub(r'\s+', '_', city.lower())
        else:
            first_part = parts[0].strip()
            normalized = re.sub(r'[^\w\s-]', '', first_part)
            normalized = re.sub(r'\s+', '_', normalized.lower())

        return normalized[:50]

    async def get_or_create_address(
        self,
        full_address: str,
        latitude: float,
        longitude: float
    ) -> Address:
        """R√©cup√®re ou cr√©e une adresse"""
        await self._ensure_connected()

        normalized = self.sanitize_address(full_address)

        # Chercher adresse existante
        existing = await self.db.address.find_first(
            where={'normalizedAddress': normalized}
        )

        if existing:
            # Mettre √† jour les coordonn√©es si elles ont chang√©
            if existing.latitude != latitude or existing.longitude != longitude:
                logger.info(f"üîÑ Mise √† jour coordonn√©es pour {normalized}: "
                          f"({existing.latitude}, {existing.longitude}) ‚Üí ({latitude}, {longitude})")
                updated = await self.db.address.update(
                    where={'id': existing.id},
                    data={
                        'latitude': latitude,
                        'longitude': longitude,
                        'fullAddress': full_address  # Mettre √† jour aussi le nom complet
                    }
                )
                return updated
            return existing

        # Cr√©er nouvelle adresse
        new_address = await self.db.address.create(
            data={
                'fullAddress': full_address,
                'normalizedAddress': normalized,
                'latitude': latitude,
                'longitude': longitude,
                'country': 'Belgium'
            }
        )

        logger.info(f"‚úÖ Adresse cr√©√©e: {normalized} (ID: {new_address.id})")
        return new_address

    async def find_address_by_normalized(self, normalized_address: str) -> Optional[Address]:
        """Trouve une adresse par son nom normalis√©"""
        await self._ensure_connected()

        return await self.db.address.find_first(
            where={'normalizedAddress': normalized_address}
        )


# ============================================================
# CLASSE : BASE DE DONN√âES AIR QUALITY (PostgreSQL)
# ============================================================

class AirQualityDB:
    """Base de donn√©es PostgreSQL pour la qualit√© de l'air"""

    def __init__(self, address: str = None):
        """
        Initialise la base Air Quality avec Prisma

        Args:
            address: Adresse pour laquelle g√©rer les donn√©es
        """
        self.current_address = address or "Bruxelles"
        self.normalized_address = AddressManager.sanitize_address(self.current_address)
        self.db: Optional[Prisma] = None
        self.address_manager = AddressManager()
        self.address_id: Optional[int] = None

        logger.info(f"‚úÖ AirQualityDB (Prisma) initialis√©e: {self.current_address}")

    async def _ensure_connected(self):
        """Assure connexion active"""
        if not self.db:
            self.db = await DatabaseClient.get_client()

    async def _ensure_address(self, lat: float = 50.8503, lon: float = 4.3517):
        """Assure que l'adresse existe"""
        if not self.address_id:
            address = await self.address_manager.get_or_create_address(
                self.current_address, lat, lon
            )
            self.address_id = address.id

    async def insert_data(self, dataframe: pd.DataFrame, lat: float = 50.8503, lon: float = 4.3517, force_update: bool = False) -> bool:
        """
        Ins√®re nouvelles donn√©es dans PostgreSQL (optimis√© batch).

        Strat√©gie: 1 SELECT pour r√©cup√©rer tous les timestamps existants,
        puis 1 create_many pour les nouveaux enregistrements.
        √âlimine le pattern N+1 (anciennement N SELECTs + N INSERTs).

        Args:
            dataframe: DataFrame avec nouvelles donn√©es
            lat: Latitude
            lon: Longitude
            force_update: Si True, met √† jour les enregistrements existants au lieu de les ignorer

        Returns:
            True si succ√®s
        """
        if dataframe is None or dataframe.empty:
            logger.error("‚ùå DataFrame vide")
            return False

        try:
            await self._ensure_connected()
            await self._ensure_address(lat, lon)

            # Pr√©parer les timestamps du DataFrame
            df_timestamps = [pd.to_datetime(row['date']) for _, row in dataframe.iterrows()]
            min_ts = min(df_timestamps)
            max_ts = max(df_timestamps)

            # 1 seul SELECT: r√©cup√©rer tous les timestamps existants pour cette plage
            existing_records = await self.db.airqualityrecord.find_many(
                where={
                    'addressId': self.address_id,
                    'timestamp': {'gte': min_ts, 'lte': max_ts}
                }
            )
            existing_ts_set = {r.timestamp for r in existing_records}
            existing_by_ts = {r.timestamp: r for r in existing_records}

            logger.info(f"üìä Batch insert: {len(dataframe)} lignes, {len(existing_ts_set)} existantes")

            # S√©parer les nouvelles donn√©es des existantes
            new_records = []
            updated = 0
            skipped = 0

            for _, row in dataframe.iterrows():
                timestamp = pd.to_datetime(row['date'])

                if timestamp in existing_ts_set:
                    if force_update:
                        # Mise √† jour individuelle (in√©vitable pour UPDATE)
                        existing = existing_by_ts[timestamp]
                        await self.db.airqualityrecord.update(
                            where={'id': existing.id},
                            data={
                                'pm10': row.get('pm10'),
                                'pm25': row.get('pm2_5'),
                                'carbonMonoxide': row.get('carbon_monoxide'),
                                'nitrogenDioxide': row.get('nitrogen_dioxide'),
                                'ozone': row.get('ozone'),
                                'sulfurDioxide': row.get('sulphur_dioxide'),
                                'dataSource': 'openmeteo'
                            }
                        )
                        updated += 1
                    else:
                        skipped += 1
                    continue

                # Ajouter aux nouveaux enregistrements pour batch insert
                new_records.append({
                    'timestamp': timestamp,
                    'addressId': self.address_id,
                    'pm10': row.get('pm10'),
                    'pm25': row.get('pm2_5'),
                    'carbonMonoxide': row.get('carbon_monoxide'),
                    'nitrogenDioxide': row.get('nitrogen_dioxide'),
                    'ozone': row.get('ozone'),
                    'sulfurDioxide': row.get('sulphur_dioxide'),
                    'dataSource': 'openmeteo'
                })

            # Batch INSERT en une seule requ√™te
            inserted = 0
            if new_records:
                result = await self.db.airqualityrecord.create_many(
                    data=new_records,
                    skip_duplicates=True
                )
                inserted = result

            logger.info(f"‚úÖ Air quality: {inserted} nouveaux, {updated} mis √† jour, {skipped} ignor√©s")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur insertion globale: {e}")
            return False

    async def get_location_data(self, address: str = None) -> pd.DataFrame:
        """
        R√©cup√®re donn√©es air quality pour une adresse

        Returns:
            DataFrame avec les donn√©es
        """
        await self._ensure_connected()

        if address is None:
            address = self.current_address

        normalized = AddressManager.sanitize_address(address)

        logger.info(f"üîç get_location_data - Recherche adresse:")
        logger.info(f"   Input address: '{address}'")
        logger.info(f"   Normalized: '{normalized}'")
        logger.info(f"   self.current_address: '{self.current_address}'")

        # Trouver l'adresse
        addr = await self.address_manager.find_address_by_normalized(normalized)

        if not addr:
            logger.warning(f"‚ö†Ô∏è Adresse non trouv√©e dans la base: '{normalized}'")
            return pd.DataFrame()

        logger.info(f"‚úÖ Adresse trouv√©e: ID={addr.id}, coords=({addr.latitude}, {addr.longitude})")

        # R√©cup√©rer les donn√©es
        records = await self.db.airqualityrecord.find_many(
            where={'addressId': addr.id},
            order={'timestamp': 'desc'}
        )

        if not records:
            logger.warning(f"‚ö†Ô∏è Aucun enregistrement pour addressId={addr.id}")
            return pd.DataFrame()

        # Convertir en DataFrame
        data = []
        for record in records:
            # Utiliser getattr pour TOUS les champs optionnels (protection contre d√©sync Prisma)
            data.append({
                'date': record.timestamp,
                'address': addr.fullAddress,
                'normalized_address': addr.normalizedAddress,
                'latitude': addr.latitude,
                'longitude': addr.longitude,

                # Polluants principaux (colonnes r√©elles de air_quality_records)
                'pm10': getattr(record, 'pm10', None),
                'pm2_5': getattr(record, 'pm25', None),
                'nitrogen_dioxide': getattr(record, 'nitrogenDioxide', None),
                'ozone': getattr(record, 'ozone', None),
                'sulphur_dioxide': getattr(record, 'sulfurDioxide', None),
                'carbon_monoxide': getattr(record, 'carbonMonoxide', None),

                # Indices AQI
                'aqi_value': getattr(record, 'aqiValue', None),
                'aqi_category': getattr(record, 'aqiCategory', None),
            })

        return pd.DataFrame(data)

    async def get_pollen_data(self, address: str = None) -> pd.DataFrame:
        """
        R√©cup√®re donn√©es pollens depuis la table pollen_records

        Returns:
            DataFrame avec les donn√©es pollens (noms utilisateur-friendly)
        """
        await self._ensure_connected()

        if address is None:
            address = self.current_address

        normalized = AddressManager.sanitize_address(address)
        addr = await self.address_manager.find_address_by_normalized(normalized)

        if not addr:
            logger.warning(f"‚ö†Ô∏è Adresse non trouv√©e pour pollens: '{normalized}'")
            return pd.DataFrame()

        # R√©cup√©rer les donn√©es pollens
        records = await self.db.pollenrecord.find_many(
            where={'addressId': addr.id},
            order={'timestamp': 'desc'}
        )

        if not records:
            logger.info(f"‚ÑπÔ∏è Aucun enregistrement pollen pour addressId={addr.id}")
            return pd.DataFrame()

        # Mapping noms botaniques ‚Üí noms utilisateur-friendly
        data = []
        for record in records:
            data.append({
                'date': record.timestamp,
                'address': addr.fullAddress,
                # Pollens avec noms courants
                'grass_pollen': getattr(record, 'graminaceae', None) or getattr(record, 'poaceae', None),
                'birch_pollen': getattr(record, 'betula', None),
                'alder_pollen': getattr(record, 'alnus', None),
                'hazel_pollen': getattr(record, 'corylus', None),
                'cypress_pollen': getattr(record, 'cupressaceae', None),
                'poplar_pollen': getattr(record, 'populus', None),
                'oak_pollen': getattr(record, 'quercus', None),
                'ash_pollen': getattr(record, 'fraxinus', None),
                'plane_pollen': getattr(record, 'platanus', None),
                'nettle_pollen': getattr(record, 'urticaceae', None),
                'mugwort_pollen': getattr(record, 'artemisia', None),
                'ragweed_pollen': getattr(record, 'ambrosia', None),
                'plantain_pollen': getattr(record, 'plantago', None),
                'chenopod_pollen': getattr(record, 'chenopod', None),
                'total_pollen': getattr(record, 'totalPollen', None),
            })

        logger.info(f"‚úÖ Pollens r√©cup√©r√©s: {len(data)} enregistrements")
        return pd.DataFrame(data)

    async def insert_pollen_data(self, dataframe: pd.DataFrame, lat: float = 50.8503, lon: float = 4.3517) -> bool:
        """
        Ins√®re donn√©es pollens dans la table pollen_records.
        Le DataFrame doit contenir les colonnes Open-Meteo: alder_pollen, birch_pollen, grass_pollen, etc.
        
        Args:
            dataframe: DataFrame avec colonnes pollen (noms Open-Meteo)
            lat: Latitude
            lon: Longitude
            
        Returns:
            True si succ√®s
        """
        # Colonnes pollen attendues depuis Open-Meteo
        pollen_cols = ['alder_pollen', 'birch_pollen', 'grass_pollen', 'olive_pollen', 
                      'ragweed_pollen', 'mugwort_pollen']
        
        # V√©rifier qu'au moins une colonne pollen existe
        available_pollen_cols = [col for col in pollen_cols if col in dataframe.columns]
        if not available_pollen_cols:
            logger.info("‚ÑπÔ∏è Aucune colonne pollen dans le DataFrame, skip insertion pollen")
            return True
        
        try:
            await self._ensure_connected()
            await self._ensure_address(lat, lon)
            
            # Pr√©parer les timestamps
            df_timestamps = [pd.to_datetime(row['date']) for _, row in dataframe.iterrows()]
            min_ts = min(df_timestamps)
            max_ts = max(df_timestamps)
            
            # R√©cup√©rer timestamps existants
            existing_records = await self.db.pollenrecord.find_many(
                where={
                    'addressId': self.address_id,
                    'timestamp': {'gte': min_ts, 'lte': max_ts}
                }
            )
            existing_ts_set = {r.timestamp for r in existing_records}
            
            # Pr√©parer nouveaux enregistrements
            new_records = []
            for _, row in dataframe.iterrows():
                timestamp = pd.to_datetime(row['date'])
                
                if timestamp in existing_ts_set:
                    continue  # Skip duplicates
                
                # Mapping Open-Meteo ‚Üí noms botaniques DB
                # Note: On utilise les noms du sch√©ma Prisma
                record = {
                    'timestamp': timestamp,
                    'addressId': self.address_id,
                    'alnus': self._safe_float(row.get('alder_pollen')),      # Alder = Aulne
                    'betula': self._safe_float(row.get('birch_pollen')),     # Birch = Bouleau
                    'graminaceae': self._safe_float(row.get('grass_pollen')), # Grass = Gramin√©es
                    'artemisia': self._safe_float(row.get('mugwort_pollen')), # Mugwort = Armoise
                    'ambrosia': self._safe_float(row.get('ragweed_pollen')),  # Ragweed = Ambroisie
                    'dataSource': 'openmeteo'
                }
                
                # Olive pollen si disponible
                if 'olive_pollen' in row:
                    # Olive n'est pas dans le sch√©ma actuel, on le stocke comme olive... √† ajouter
                    pass
                
                new_records.append(record)
            
            # Batch insert
            inserted = 0
            if new_records:
                result = await self.db.pollenrecord.create_many(
                    data=new_records,
                    skip_duplicates=True
                )
                inserted = result
            
            logger.info(f"‚úÖ Pollens: {inserted} nouveaux enregistrements (sur {len(dataframe)} lignes)")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur insertion pollens: {e}")
            return False
    
    def _safe_float(self, value) -> Optional[float]:
        """Convertit une valeur en float, retourne None si NaN ou invalide"""
        import math
        if value is None:
            return None
        try:
            f = float(value)
            if math.isnan(f):
                return None
            return f
        except (ValueError, TypeError):
            return None

    async def get_date_range(self, address: str = None) -> Optional[Dict]:
        """
        R√©cup√®re l'intervalle de dates pour une adresse
        OPTIMIS√â: 1 seule requ√™te agr√©g√©e au lieu de 3 requ√™tes s√©par√©es

        Returns:
            dict avec start_date et end_date ou None
        """
        await self._ensure_connected()

        if address is None:
            address = self.current_address

        normalized = AddressManager.sanitize_address(address)
        addr = await self.address_manager.find_address_by_normalized(normalized)

        if not addr:
            return None

        # OPTIMISATION: 1 seule requ√™te agr√©g√©e au lieu de 3 requ√™tes
        result = await self.db.query_raw('''
            SELECT 
                MIN(timestamp) as start_date,
                MAX(timestamp) as end_date,
                COUNT(*) as total_records
            FROM air_quality_records 
            WHERE address_id = $1
        ''', addr.id)

        if not result or result[0]['total_records'] == 0:
            return None

        return {
            'start_date': result[0]['start_date'],
            'end_date': result[0]['end_date'],
            'total_records': result[0]['total_records']
        }


# ============================================================
# CLASSE : BASE DE DONN√âES M√âT√âO (PostgreSQL)
# ============================================================

class WeatherDB:
    """Base de donn√©es PostgreSQL pour la m√©t√©o"""

    def __init__(self, address: str = None):
        """
        Initialise la base Weather avec Prisma

        Args:
            address: Adresse pour laquelle g√©rer les donn√©es
        """
        self.current_address = address or "Bruxelles"
        self.normalized_address = AddressManager.sanitize_address(self.current_address)
        self.db: Optional[Prisma] = None
        self.address_manager = AddressManager()
        self.address_id: Optional[int] = None

        logger.info(f"‚úÖ WeatherDB (Prisma) initialis√©e: {self.current_address}")

    async def _ensure_connected(self):
        if not self.db:
            self.db = await DatabaseClient.get_client()

    async def _ensure_address(self, lat: float = 50.8503, lon: float = 4.3517):
        if not self.address_id:
            address = await self.address_manager.get_or_create_address(
                self.current_address, lat, lon
            )
            self.address_id = address.id

    async def insert_data(self, dataframe: pd.DataFrame, lat: float = 50.8503, lon: float = 4.3517, force_update: bool = False) -> bool:
        """Ins√®re donn√©es m√©t√©o dans PostgreSQL (optimis√© batch)."""
        if dataframe is None or dataframe.empty:
            logger.error("‚ùå DataFrame m√©t√©o vide")
            return False

        try:
            await self._ensure_connected()
            await self._ensure_address(lat, lon)

            # Pr√©parer les timestamps du DataFrame
            df_timestamps = [pd.to_datetime(row['date']) for _, row in dataframe.iterrows()]
            min_ts = min(df_timestamps)
            max_ts = max(df_timestamps)

            # 1 seul SELECT: r√©cup√©rer tous les timestamps existants pour cette plage
            existing_records = await self.db.weatherrecord.find_many(
                where={
                    'addressId': self.address_id,
                    'timestamp': {'gte': min_ts, 'lte': max_ts}
                }
            )
            existing_ts_set = {r.timestamp for r in existing_records}
            existing_by_ts = {r.timestamp: r for r in existing_records}

            logger.info(f"üìä M√©t√©o batch: {len(dataframe)} lignes, {len(existing_ts_set)} existantes")

            new_records = []
            updated = 0
            skipped = 0

            for _, row in dataframe.iterrows():
                timestamp = pd.to_datetime(row['date'])

                if timestamp in existing_ts_set:
                    if force_update:
                        existing = existing_by_ts[timestamp]
                        await self.db.weatherrecord.update(
                            where={'id': existing.id},
                            data={
                                'temperature': row.get('temperature'),
                                'feelsLike': row.get('feels_like'),
                                'humidity': row.get('humidity'),
                                'pressure': row.get('pressure'),
                                'windSpeed': row.get('wind_speed'),
                                'windDirection': row.get('wind_direction'),
                                'windGusts': row.get('wind_gusts'),
                                'cloudCover': row.get('cloud_cover'),
                                'precipitation1h': row.get('rain'),
                                'weatherCode': row.get('weather_code'),
                                'visibility': row.get('visibility'),
                                'dataSource': 'meteosource'
                            }
                        )
                        updated += 1
                    else:
                        skipped += 1
                    continue

                new_records.append({
                    'timestamp': timestamp,
                    'addressId': self.address_id,
                    'temperature': row.get('temperature'),
                    'feelsLike': row.get('feels_like'),
                    'humidity': row.get('humidity'),
                    'pressure': row.get('pressure'),
                    'windSpeed': row.get('wind_speed'),
                    'windDirection': row.get('wind_direction'),
                    'windGusts': row.get('wind_gusts'),
                    'cloudCover': row.get('cloud_cover'),
                    'precipitation1h': row.get('rain'),
                    'weatherCode': row.get('weather_code'),
                    'visibility': row.get('visibility'),
                    'dataSource': 'meteosource'
                })

            # Batch INSERT
            inserted = 0
            if new_records:
                result = await self.db.weatherrecord.create_many(
                    data=new_records,
                    skip_duplicates=True
                )
                inserted = result

            logger.info(f"‚úÖ M√©t√©o: {inserted} nouveaux, {updated} mis √† jour, {skipped} ignor√©s")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur insertion m√©t√©o globale: {e}")
            return False

    async def save_hourly_weather(self, address: str, lat: float, lon: float, hourly_df: pd.DataFrame) -> bool:
        """
        Sauvegarde donn√©es m√©t√©o horaires (historiques ou pr√©visions)
        Compatible avec download_weather.py

        Args:
            address: Adresse
            lat: Latitude
            lon: Longitude
            hourly_df: DataFrame avec colonnes: date, temperature, etc.

        Returns:
            True si succ√®s
        """
        # Utiliser insert_data existant qui g√®re d√©j√† les donn√©es horaires
        return await self.insert_data(hourly_df, lat, lon)

    async def save_current_weather(self, address: str, lat: float, lon: float, current_data: Dict) -> bool:
        """
        Sauvegarde m√©t√©o actuelle

        Args:
            address: Adresse
            lat: Latitude
            lon: Longitude
            current_data: Dict avec temp√©rature, vent, etc.

        Returns:
            True si succ√®s
        """
        try:
            await self._ensure_connected()
            await self._ensure_address(lat, lon)

            timestamp = datetime.now()

            # V√©rifier si existe d√©j√†
            existing = await self.db.weatherrecord.find_first(
                where={
                    'addressId': self.address_id,
                    'timestamp': timestamp
                }
            )

            if existing:
                logger.info("M√©t√©o actuelle d√©j√† pr√©sente")
                return True

            # Cr√©er enregistrement
            await self.db.weatherrecord.create(
                data={
                    'timestamp': timestamp,
                    'addressId': self.address_id,
                    'temperature': current_data.get('temperature'),
                    'feelsLike': current_data.get('feels_like'),
                    'humidity': current_data.get('humidity'),
                    'pressure': current_data.get('pressure'),
                    'windSpeed': current_data.get('wind_speed'),
                    'windDirection': current_data.get('wind_direction'),
                    'windGusts': current_data.get('wind_gusts'),
                    'cloudCover': current_data.get('cloud_cover'),
                    'precipitation1h': current_data.get('rain'),
                    'weatherCode': current_data.get('weather_code'),
                    'visibility': current_data.get('visibility'),
                    'dataSource': 'open-meteo'
                }
            )

            logger.info("‚úÖ M√©t√©o actuelle sauvegard√©e")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde m√©t√©o actuelle: {e}")
            return False

    async def save_daily_weather(self, address: str, lat: float, lon: float, daily_df: pd.DataFrame) -> bool:
        """
        Sauvegarde pr√©visions journali√®res

        Args:
            address: Adresse
            lat: Latitude
            lon: Longitude
            daily_df: DataFrame avec pr√©visions journali√®res

        Returns:
            True si succ√®s
        """
        # Pour les pr√©visions journali√®res, on peut les convertir en format horaire
        # en utilisant le midi de chaque jour, ou ignorer si pas n√©cessaire
        logger.info(f"Sauvegarde de {len(daily_df)} pr√©visions journali√®res (conversion en horaire)")
        return await self.insert_data(daily_df, lat, lon)

    async def get_latest_current_weather(self, address: str = None) -> Optional[Dict]:
        """R√©cup√®re la m√©t√©o actuelle la plus r√©cente"""
        await self._ensure_connected()

        if address is None:
            address = self.current_address

        normalized = AddressManager.sanitize_address(address)
        addr = await self.address_manager.find_address_by_normalized(normalized)

        if not addr:
            return None

        record = await self.db.weatherrecord.find_first(
            where={'addressId': addr.id},
            order={'timestamp': 'desc'}
        )

        if not record:
            return None

        return {
            'temperature': record.temperature,
            'feels_like': record.feelsLike,
            'humidity': record.humidity,
            'pressure': record.pressure,
            'wind_speed': record.windSpeed,
            'wind_direction': record.windDirection,
            'timestamp': record.timestamp
        }

    async def get_temperature_statistics(self, address: str = None) -> Dict:
        """
        Calcule statistiques de temp√©rature
        OPTIMIS√â: Calcul c√¥t√© base de donn√©es (AVG, MIN, MAX)
        """
        await self._ensure_connected()

        if address is None:
            address = self.current_address

        normalized = AddressManager.sanitize_address(address)
        addr = await self.address_manager.find_address_by_normalized(normalized)

        if not addr:
            return {}

        # OPTIMISATION: Calculs agr√©g√©s en SQL
        result = await self.db.query_raw('''
            SELECT 
                AVG(temperature) as avg_temp,
                MIN(temperature) as min_temp,
                MAX(temperature) as max_temp,
                COUNT(*) as total_records
            FROM weather_records 
            WHERE address_id = $1 AND temperature IS NOT NULL
        ''', addr.id)

        if not result or result[0]['total_records'] == 0:
            return {}
            
        row = result[0]

        return {
            'avg_temp': row['avg_temp'],
            'min_temp': row['min_temp'],
            'max_temp': row['max_temp'],
            'total_records': row['total_records']
        }

    async def get_hourly_forecast(self, address: str = None, hours: int = 24) -> pd.DataFrame:
        """R√©cup√®re pr√©visions horaires"""
        await self._ensure_connected()

        if address is None:
            address = self.current_address

        normalized = AddressManager.sanitize_address(address)
        addr = await self.address_manager.find_address_by_normalized(normalized)

        if not addr:
            return pd.DataFrame()

        records = await self.db.weatherrecord.find_many(
            where={'addressId': addr.id},
            order={'timestamp': 'desc'},
            take=hours
        )

        if not records:
            return pd.DataFrame()

        data = []
        for record in records:
            data.append({
                'date': record.timestamp,
                'address': addr.fullAddress,
                'temperature': record.temperature,
                'feels_like': record.feelsLike,
                'humidity': record.humidity,
                'pressure': record.pressure,
                'wind_speed': record.windSpeed,
                'wind_direction': record.windDirection,
                'wind_gusts': record.windGusts,
                'cloud_cover': record.cloudCover,
                'rain': record.precipitation1h,
                'weather_code': record.weatherCode,
                'visibility': record.visibility
            })

        df = pd.DataFrame(data)
        df = df.sort_values('date')
        return df


# ============================================================
# GESTIONNAIRE DE BASES (Compatible avec l'ancien code)
# ============================================================

class DatabaseManager:
    """Gestionnaire compatible avec l'ancien code SQLite"""

    @staticmethod
    def sanitize_address(address: str) -> str:
        """Wrapper pour compatibilit√©"""
        return AddressManager.sanitize_address(address)

    @staticmethod
    async def list_all_databases(db_type: str = 'air_quality') -> List[Dict]:
        """
        Liste toutes les adresses disponibles dans PostgreSQL
        OPTIMIS√â: Utilise 1 seule requ√™te SQL avec JOIN au lieu de boucle N+1
        """
        db = await DatabaseClient.get_client()
        databases = []

        try:
            if db_type == 'air_quality':
                # Requ√™te optimis√©e pour Air Quality
                # R√©cup√®re adresses + stats en 1 seule passe
                query = '''
                    SELECT 
                        a.id, a.normalized_address, a.updated_at,
                        COUNT(r.id) as count,
                        MIN(r.timestamp) as min_date,
                        MAX(r.timestamp) as max_date
                    FROM addresses a
                    LEFT JOIN air_quality_records r ON a.id = r.address_id
                    GROUP BY a.id
                    HAVING COUNT(r.id) > 0
                    ORDER BY a.created_at DESC
                '''
                results = await db.query_raw(query)
                
                for row in results:
                    databases.append({
                        'path': f'postgresql://{row["normalized_address"]}',
                        'type': 'air_quality',
                        'size': 0, # N/A PostgreSQL
                        'modified': row['updated_at'],
                        'address': row['normalized_address'],
                        'records': row['count'],
                        'date_range': f"{row['min_date'] or 'N/A'} ‚Üí {row['max_date'] or 'N/A'}"
                    })
                    
            else:
                # Requ√™te optimis√©e pour Weather
                query = '''
                    SELECT 
                        a.id, a.normalized_address, a.updated_at,
                        COUNT(r.id) as count,
                        MIN(r.timestamp) as min_date,
                        MAX(r.timestamp) as max_date
                    FROM addresses a
                    LEFT JOIN weather_records r ON a.id = r.address_id
                    GROUP BY a.id
                    HAVING COUNT(r.id) > 0
                    ORDER BY a.created_at DESC
                '''
                results = await db.query_raw(query)
                
                for row in results:
                    databases.append({
                        'path': f'postgresql://{row["normalized_address"]}',
                        'type': 'weather',
                        'size': 0,
                        'modified': row['updated_at'],
                        'address': row['normalized_address'],
                        'records': row['count'],
                        'date_range': f"{row['min_date'] or 'N/A'} ‚Üí {row['max_date'] or 'N/A'}"
                    })

            return databases
            
        except Exception as e:
            # Fallback en cas d'erreur de requ√™te (table manquante, etc)
            import logging
            logging.getLogger(__name__).error(f"‚ùå Erreur list_all_databases optimis√©: {e}")
            return []


# ============================================================
# GESTIONNAIRE DE STATIONS
# ============================================================

class StationManager:
    """Gestion des stations de mesure avec support PostGIS"""

    def __init__(self):
        self.db: Optional[Prisma] = None

    async def _ensure_connected(self):
        """Assure la connexion √† la base de donn√©es"""
        if not self.db:
            self.db = await DatabaseClient.get_client()

    async def get_all_stations(self, station_type: Optional[str] = None, active_only: bool = True) -> List[Dict]:
        """
        R√©cup√®re toutes les stations de mesure

        Args:
            station_type: Type de station ('air_quality', 'weather') ou None pour toutes
            active_only: Si True, ne retourne que les stations actives

        Returns:
            Liste de dictionnaires avec les informations des stations
        """
        await self._ensure_connected()

        where_clause = {}
        if station_type:
            where_clause['stationType'] = station_type
        if active_only:
            where_clause['isActive'] = True

        stations = await self.db.station.find_many(
            where=where_clause if where_clause else None,
            order={'stationName': 'asc'}
        )

        result = []
        for station in stations:
            # R√©cup√©rer le nombre de mesures pour cette station
            air_quality_count = await self.db.airqualityrecord.count(
                where={'stationId': station.id}
            )
            weather_count = await self.db.weatherrecord.count(
                where={'stationId': station.id}
            )

            # R√©cup√©rer la derni√®re mesure
            last_measurement = None
            if station.stationType == 'air_quality':
                last_record = await self.db.airqualityrecord.find_first(
                    where={'stationId': station.id},
                    order={'timestamp': 'desc'}
                )
                if last_record:
                    last_measurement = last_record.timestamp
            else:
                last_record = await self.db.weatherrecord.find_first(
                    where={'stationId': station.id},
                    order={'timestamp': 'desc'}
                )
                if last_record:
                    last_measurement = last_record.timestamp

            result.append({
                'id': station.id,
                'station_code': station.stationCode,
                'station_name': station.stationName,
                'station_type': station.stationType,
                'latitude': station.latitude,
                'longitude': station.longitude,
                'elevation': station.elevation,
                'metadata': station.metadata,
                'is_active': station.isActive,
                'air_quality_records': air_quality_count,
                'weather_records': weather_count,
                'last_measurement': last_measurement,
                'created_at': station.createdAt,
                'updated_at': station.updatedAt
            })

        return result

    async def get_stations_near_location(
        self,
        latitude: float,
        longitude: float,
        radius_km: float = 10.0,
        station_type: Optional[str] = None
    ) -> List[Dict]:
        """
        R√©cup√®re les stations dans un rayon donn√© autour d'une position
        Utilise PostGIS pour le calcul de distance

        Args:
            latitude: Latitude du point central
            longitude: Longitude du point central
            radius_km: Rayon de recherche en kilom√®tres
            station_type: Type de station ou None pour toutes

        Returns:
            Liste de stations tri√©es par distance
        """
        await self._ensure_connected()

        # Utiliser une requ√™te SQL brute avec PostGIS pour calculer les distances
        # ST_Distance retourne la distance en degr√©s, conversion approximative: 1¬∞ ‚âà 111 km
        radius_degrees = radius_km / 111.0

        where_parts = [f"ST_DWithin(geom, ST_SetSRID(ST_MakePoint({longitude}, {latitude}), 4326), {radius_degrees})"]

        if station_type:
            where_parts.append(f"station_type = '{station_type}'")

        where_clause = " AND ".join(where_parts)

        query = f"""
            SELECT
                id, station_code, station_name, station_type,
                latitude, longitude, elevation, metadata, is_active,
                created_at, updated_at,
                ST_Distance(
                    geom::geography,
                    ST_SetSRID(ST_MakePoint({longitude}, {latitude}), 4326)::geography
                ) / 1000 as distance_km
            FROM stations
            WHERE {where_clause}
            ORDER BY distance_km ASC
        """

        stations = await self.db.query_raw(query)

        result = []
        for station in stations:
            # R√©cup√©rer le nombre de mesures
            air_quality_count = await self.db.airqualityrecord.count(
                where={'stationId': station['id']}
            )
            weather_count = await self.db.weatherrecord.count(
                where={'stationId': station['id']}
            )

            result.append({
                'id': station['id'],
                'station_code': station['station_code'],
                'station_name': station['station_name'],
                'station_type': station['station_type'],
                'latitude': station['latitude'],
                'longitude': station['longitude'],
                'elevation': station['elevation'],
                'metadata': station['metadata'],
                'is_active': station['is_active'],
                'distance_km': round(station['distance_km'], 2),
                'air_quality_records': air_quality_count,
                'weather_records': weather_count,
                'created_at': station['created_at'],
                'updated_at': station['updated_at']
            })

        return result

    async def get_station_by_code(self, station_code: str) -> Optional[Dict]:
        """
        R√©cup√®re une station par son code

        Args:
            station_code: Code unique de la station

        Returns:
            Dictionnaire avec les informations de la station ou None
        """
        await self._ensure_connected()

        station = await self.db.station.find_unique(
            where={'stationCode': station_code}
        )

        if not station:
            return None

        air_quality_count = await self.db.airqualityrecord.count(
            where={'stationId': station.id}
        )
        weather_count = await self.db.weatherrecord.count(
            where={'stationId': station.id}
        )

        return {
            'id': station.id,
            'station_code': station.stationCode,
            'station_name': station.stationName,
            'station_type': station.stationType,
            'latitude': station.latitude,
            'longitude': station.longitude,
            'elevation': station.elevation,
            'metadata': station.metadata,
            'is_active': station.isActive,
            'air_quality_records': air_quality_count,
            'weather_records': weather_count,
            'created_at': station.createdAt,
            'updated_at': station.updatedAt
        }

    async def create_station(
        self,
        station_code: str,
        station_name: str,
        station_type: str,
        latitude: float,
        longitude: float,
        elevation: Optional[float] = None,
        metadata: Optional[Dict] = None
    ) -> Dict:
        """
        Cr√©e une nouvelle station

        Args:
            station_code: Code unique de la station
            station_name: Nom de la station
            station_type: Type ('air_quality' ou 'weather')
            latitude: Latitude
            longitude: Longitude
            elevation: Altitude (optionnel)
            metadata: M√©tadonn√©es additionnelles (optionnel)

        Returns:
            Dictionnaire avec les informations de la station cr√©√©e
        """
        await self._ensure_connected()

        # Cr√©er le point PostGIS
        # ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)
        station = await self.db.station.create(
            data={
                'stationCode': station_code,
                'stationName': station_name,
                'stationType': station_type,
                'latitude': latitude,
                'longitude': longitude,
                'elevation': elevation,
                'metadata': metadata,
                'isActive': True
            }
        )

        # Mettre √† jour la g√©om√©trie PostGIS avec une requ√™te raw
        await self.db.execute_raw(
            f"""
            UPDATE stations
            SET geom = ST_SetSRID(ST_MakePoint({longitude}, {latitude}), 4326)
            WHERE id = {station.id}
            """
        )

        return {
            'id': station.id,
            'station_code': station.stationCode,
            'station_name': station.stationName,
            'station_type': station.stationType,
            'latitude': station.latitude,
            'longitude': station.longitude,
            'elevation': station.elevation,
            'metadata': station.metadata,
            'is_active': station.isActive
        }


# ============================================================
# EXPORT
# ============================================================

__all__ = [
    'DatabaseClient',
    'AddressManager',
    'AirQualityDB',
    'WeatherDB',
    'DatabaseManager',
    'StationManager'
]
